{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h1>Sound it Out</h1>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><i>\n",
    "Diana Chou, Max Scribner, Lewis Qualkenbush<br>   \n",
    "CMSC320 Final Tutorial<br>\n",
    "Fall 2019, John Dickerson\n",
    "</i></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='th_sound_comic.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without any given context, would you able to say a word properly just by reading it? To what extent can we predict the pronounciation of a word from its spelling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Table of Contents</h3><h4><ol><li>\n",
    "    \n",
    "[Introduction](#0)</li><br><li>\n",
    "    \n",
    "[Goal](#1)</li><br><li>\n",
    "\n",
    "[Data Wrangling](#2)<ul><li>\n",
    "    \n",
    "[Cleaning the Data](#2-1)</li><li>\n",
    "\n",
    "[Finding Unique Characters for Analysis](#2-2)</li><li>\n",
    "\n",
    "[Finding the Distributions of Letters and Phonemes Within the Data](#2-3)</li></ul><li>\n",
    "\n",
    "[Data Exploration and Analysis](#3)<ul><li>\n",
    "    \n",
    "[Intra-word Co-occurence Map](#3-1)</li><li>\n",
    "\n",
    "[Finding Clusters Within Words](#3-2)</li><li>\n",
    "\n",
    "[Preparing Data with Clustering](#3-3)</li><li>\n",
    "\n",
    "[Intra-cluster Co-occurence Map](#3-4)</li><li>\n",
    "\n",
    "[Incompatible Clustering Analysis](#3-5)</li></ul><li>\n",
    "   \n",
    "[Decision Tree Model](#4)</li><br><li>\n",
    "\n",
    "[Ideas for Further Improvement](#5)</li><br><li>\n",
    "\n",
    "[Conclusion](#6)</li></ol></h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A primary goal highlighted in early education is the development of basic speaking and reading skills. Despite this, in 2013, only about one-third of a nationwide sample of fourth graders read at or above the proficient level (<a href=\"https://www.nationsreportcard.gov/reading_math_2013/#/state-performance\">National Center for Education Statistics</a>, 2013). Reading words and being able to pronounce them is a complex process involving the coordination of multiple skills and linguistic systems in order to result in fluent behaviors. It is important for researchers to investigate the skills and systems involved in reading and speaking in order to be able to design educational programs and learning environments that foster growth in these skills and systems. Reading and speaking involves the coordination of multiple systems including orthography and phonology. \n",
    "\n",
    "#### Orthography \n",
    "\n",
    "Orthography is a set of rules for writing in a language. The word “orthography” is based on the Greek words that mean “right writing”. Every written language (used for education, publishing literature, legal documents, etc.) has some kind of an orthography, which develops over the years as people use it for communication, and literature. The rules are usually written by linguists who know the language well and enjoy the respect of the people who speak the language, and they try to balance general accepted usage and a rigid standard. Formalizing an orthography is a part of larger activity called language planning. \n",
    "\n",
    "Orthographic processing involves the visual structure of a word (or string of letters). A majority of the time, you depend on the orthographic representation to quickly know that “cat” is a real word while “cta” is not. Research suggests that the ability to automatically orthographically process strings of letters as words depends on the <a href=\"https://en.wikipedia.org/wiki/Visual_word_form_area\">visual word form area</a> (VWFA) system, which may develop over time with experience with words.\n",
    "\n",
    "#### Phonology\n",
    "\n",
    "Phonology is an area of linguistics that explores the sounds of a language. Each language uses a different set of sounds, rules for which sounds follow after one another, what is perceived as syllables, what is the stress of the word, and so on. It is usually researched as part of scientific linguistics and grammar. \n",
    "\n",
    "Phonological processing involves the sounds of a language, called phonemes. A majority of the time, you depend on phonological processing to know that “cat” says kuh – ah – tuh. Tasks that delve into phonological processing, such as rhyming games or phoneme manipulation (say “cat” without the kuh), have proven to be some of the strongest correlates and predictors of pronouncing a word. \n",
    "\n",
    "##### In order to learn how to read a word and pronounce it correctly, the phonemes that one knows from spoken language (phonology) must be mapped on to the printed letters on the page (orthography).\n",
    "\n",
    "<br><center><img src=\"phonemic_chart.jpg\" width=\"70%\"></center>\n",
    "\n",
    "#### Phonology != Phonetics\n",
    "\n",
    "There are two ways in which we can transcribe speech. Phonemic transcription, also sometimes known as ‘broad’ transcription, involves representing speech using just a unique symbol for each phoneme of the language. The other way we can transcribe speech is using phonetic transcription, also sometimes known as ‘narrow’ transcription, involves representing additional details about the contextual variations in pronunciation that occur in normal speech. \n",
    "Phonemic and phonetic transcription both have their purposes. The goal of a phonemic transcription is to record the phonemes as ‘mental categories’ that a speaker uses, rather than the actual spoken variants of those phonemes that are produced in the context of a particular word. Phonetic transcription on the other hand specifies the finer details of how sounds are actually made, and is based on the surrounding context and phonological rules in specific dialects and languages. Thus, the phonemic representation of a word is a more accurate representation on how to say a word and predict its pronunciation based on how it is spelled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "### Our Goal? <br><br> Create a model that takes in the spelling of a word and output a prediction at its pronunciation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "### Data Wrangling\n",
    "#### Downlaoding data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found the data on a website containing the “top 1000 words” in the English language, and their transcriptions. Code was written which scraped the list of “top 1000 words” and went to their associated web page, finding the transcription in the HTML using BeautifulSoup, python’s requests library, and its regex library.\n",
    "\n",
    "You can see that code <a href=\"get_british.html\">here</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2-1'></a>\n",
    "#### Cleaning the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some cleanup of the data was required before the analysis step. There are stress marking ('), long vowel (:), and secondary stress marking (ˌ) characters included in the data that are not important to our analysis, so these are cut out of the word transcriptions. Our code does not work with capitalized letters, so words with capitalized letters had to be changed to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV into a new dataframe\n",
    "transcriptions = pd.read_csv(\"transcriptions/british.csv\")\n",
    "\n",
    "# We want to remove characters we do not want to perform analysis on\n",
    "transcriptions = transcriptions.replace(to_replace='\\'|:|ˌ', value = '', regex = True)\n",
    "transcriptions['word'] = transcriptions['word'].str.lower()\n",
    "transcriptions = transcriptions[['word', 'transcription']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2-2'></a>\n",
    "#### Find Unique Characters for Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of mapping spellings of words to their pronunciations, we needed to explore the data and find all of the unique characters that are within the set of orthographic symbols and phonological symbols. This will be helpful for when we are mapping later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chars(string):\n",
    "    chars = set()\n",
    "    \n",
    "    for char in string:\n",
    "        chars.add(char.lower())\n",
    "        \n",
    "    return chars\n",
    "\n",
    "orth_chars = set()\n",
    "phon_chars = set()\n",
    "\n",
    "# Get unique characters for both letters and phonemes from the dataset\n",
    "for index, row in transcriptions.iterrows():\n",
    "    orth_chars = orth_chars.union(get_chars(row[\"word\"]))\n",
    "    \n",
    "    phon_chars = phon_chars.union(get_chars(row[\"transcription\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'z', 'p', 'l', 'u', 's', 'v', 'i', 'j', 'h', 'c', 'm', 'b', 'y', 'a', 't', 'o', 'f', 'e', 'k', 'g', 'w', 'd', 'x', 'q', 'r', 'n'}\n",
      "{'ɔ', 'z', 'l', 'ð', 'θ', 'j', 'h', 'a', 't', 'k', 'ɑ', 'g', 'ʒ', 'ɒ', 'p', 'u', 's', 'v', 'i', 'ʃ', 'm', 'æ', 'b', 'ʌ', 'f', 'e', 'ʊ', 'ŋ', 'ɪ', 'ə', 'd', 'w', 'ɜ', 'r', 'n'}\n"
     ]
    }
   ],
   "source": [
    "print(orth_chars)\n",
    "print(phon_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2-3'></a>\n",
    "#### Finding the Distributions of Letters and Phonemes Within the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another aspect of the data that will be important for our statistical calculations later on is the occurrence of each orthographic and phonological character within the dataset overall. This was as simple as running through each row and keeping a count of appearances of that specific character. This value was then normalized on a scale from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "orth_list = list(orth_chars)\n",
    "phon_list = list(phon_chars)\n",
    "orth_list.sort()\n",
    "phon_list.sort()\n",
    "\n",
    "orth_prob = {k: 0 for k in orth_list}\n",
    "phon_prob = {k: 0 for k in phon_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of times a letter or phoneme appears within the dataset\n",
    "for index, row in transcriptions.iterrows():\n",
    "    for orth in list(row['word']):\n",
    "        orth_prob[orth] += 1\n",
    "    \n",
    "    for phon in list(row['transcription']):\n",
    "        phon_prob[phon] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the distribution values of letters and phonemes\n",
    "orth_prob_max = max(orth_prob.values())\n",
    "phon_prob_max = max(phon_prob.values())\n",
    "\n",
    "orth_prob = {k: (v/orth_prob_max) for k, v in orth_prob.items()}\n",
    "phon_prob = {k: (v/phon_prob_max) for k, v in phon_prob.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "### Data Exploration and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3-1'></a>\n",
    "#### Intra-word Co-occurence Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we fit a model to predict how words are pronounced, we found it a good idea to see if some statistics could prove relations between certain words and letters. At its simplest level, we are finding how likely it is that a letter and phoneme are found within the same word, transcription pair. We have a matrix with letters and phonemes where each value holds the weight of that pairs co-occurrence. The higher the value, the more likely we have found that pair to co-occur within a word or transcription. This value is weighted by both the length of the word and transcription as well as the distribution of those characters within the overall dataset. This prevents letters like ‘q’ and ‘z’ from having very small co-occurrence given they are much rarer in orthographic British English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm_orth_dict = {k: v for v, k in enumerate(orth_list)}\n",
    "hm_phon_dict = {k: v for v, k in enumerate(phon_list)}\n",
    "\n",
    "# Create a new dataframe to hold the values for the heatmap\n",
    "heatmap_frame = pd.DataFrame(index=[i for i in range(0, len(hm_phon_dict))], \n",
    "                             columns=[i for i in range(0, len(hm_orth_dict))], \n",
    "                             data=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-17d54f91bc05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;31m# Add this weighted value to the heatmap for the given pair\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mheatmap_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhm_phon_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphon\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhm_orth_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0morth\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mweighted_val\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdist_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_setitem_indexer\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_setter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_tuple\u001b[0;34m(self, key, is_setter)\u001b[0m\n\u001b[1;32m    257\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Too many indexers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m                 \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_setter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_setter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m                 \u001b[0mkeyidx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyidx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter, raise_missing)\u001b[0m\n\u001b[1;32m   1269\u001b[0m                 \u001b[0;31m# When setting, missing keys are not allowed, even with .loc:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"raise_missing\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_setter\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1271\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1272\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m         self._validate_read_indexer(\n\u001b[0;32m-> 1078\u001b[0;31m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1079\u001b[0m         )\n\u001b[1;32m   1080\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_axis_number\u001b[0;34m(cls, axis)\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_AXIS_ALIASES\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loop through each word, transcription pair in the dataset\n",
    "for index, row in transcriptions.iterrows():\n",
    "    word_length = len(row['word'])\n",
    "    transcription_length = len(row['transcription'])\n",
    "    \n",
    "    # Calculate the weight we want to add to that pairing given the current lengths\n",
    "    # of the word and transcription\n",
    "    weight = word_length * transcription_length\n",
    "    weighted_val = float(1/weight)\n",
    "    \n",
    "    for orth in list(row['word']):\n",
    "        for phon in list(row['transcription']):\n",
    "            # Calculate the weighting based on the distribution of a given letter\n",
    "            # and phoneme\n",
    "            dist_weight = float(1/(orth_prob[orth] * phon_prob[phon]))\n",
    "            \n",
    "            # Add this weighted value to the heatmap for the given pair\n",
    "            heatmap_frame.loc[[hm_phon_dict[phon]],[hm_orth_dict[orth]]] += (weighted_val * dist_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_frame.index = phon_list\n",
    "heatmap_frame.columns = orth_list\n",
    "\n",
    "# Normalize the values in the dataframe so they are on a scale from zero to one\n",
    "max_val = max(heatmap_frame.max())\n",
    "heatmap_frame = heatmap_frame.div(max_val)\n",
    "\n",
    "heatmap_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the dataframe showing the co-occurence values of each phoneme, letter pair\n",
    "plt.subplots(figsize=(20,15))\n",
    "ax = sns.heatmap(heatmap_frame, cmap=\"Greens\")\n",
    "plt.yticks(rotation=0, va=\"center\", ha=\"center\")\n",
    "ax.set_title('Intra-Word Co-Occurence of Phoneme and Letter'), ax.set_ylabel('Phoneme'), ax.set_xlabel('Letter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the letter of greatest co-occurence for each phoneme\n",
    "phon_comax = heatmap_frame.idxmax(axis=1).to_frame()\n",
    "phon_comax.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the phoneme of greatest co-occurence for each letter\n",
    "orth_comax = heatmap_frame.idxmax(axis=0).to_frame()\n",
    "orth_comax.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the heatmap is created, we can find mappings between the letters and phonemes based on the co-occurrence values. For our purposes here, we are finding the phoneme with the largest co-occurrence value for each letter and the letter with the largest co-occurrence value for each phoneme. This mapping worked rather well from a phonological point of view, where for the most part the consonants in the english alphabet map closely to how we are most likely to pronounce that latter on its own. However, for some of the vowels, they were mapped to consonants, which suggests that our analysis needs to be more granular than intra-word co-occurrence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3-2'></a>\n",
    "#### Finding Clusters Within Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way in which we can increase the granularity of our analysis is by digging deeper into the words themselves. We want to make sure that letters are mapped correctly to phonemes in the transcription. Before, we were including relationships between letters and phonemes that may have been in completely different parts of the word. Words are characterized by a specific character ordering, but we can also analyze clusters of these characters within the word. <br> <br>\n",
    "A cluster is a grouping of different characters within a specific category. A word like ‘enjoy’ would appear as a group of clusters like [‘e’, ‘nj’, ‘oy’] where each cluster within this listing is either a cluster of consonants or vowels. The transcription equivalent of this word would look like ‘ɪndʒɔɪ’ = [‘ɪ’, ‘ndʒ’, ‘ɔɪ’]. We can find the clustering of these words and transcriptions by running them through a function that first assigns each character a vowel or consonant attribute and then runs through that word and finds local groups of these vowels or consonants. We are attempting to map clusters from the word to the transcription. It is worth noting that a character having a consonant or vowel attribute is dependent on information we already know about both orthographic and phonological British English. This data has been added manually to the script. <br> <br>\n",
    "Another topic of interest is the fact that in the spelling of a word in British English, the character ‘y’ is considered a semivowel in that it can represent both the IPA ‘j’ consonant at the beginning of words like ‘yes’ or what is considered the nucleus vowel of a syllable. In this way, within the spelling of a word, ‘y’ can be considered both a vowel and a consonant, and we have added code that accounts for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual listing of consonants and vowels for orthrographical British English\n",
    "orth_consonants = ['l', 't', 'f', 'm', 'n', 'v', 'p', 'h', 'j', 'w', 'r', 'b', 'x', 'c', 'q', 'z',  'g', 's', 'k', 'd', 'y']\n",
    "orth_consonants.sort()\n",
    "orth_vowels = ['i', 'a', 'o', 'e', 'u', 'y']\n",
    "orth_vowels.sort()\n",
    "\n",
    "# Manual listing of consonants and vowels for phonological British English\n",
    "phon_consonants = ['t', 'f', 'd', 'p', 'h', 'ʃ', 'r', 'b', 'θ', 'ŋ', 'g', 'k', 'ʒ', 'l', 'm', 'n', 'v', 'j', 'w', 'ð', 's', 'z']\n",
    "phon_consonants.sort()\n",
    "phon_vowels = ['ɑ', 'a', 'ɜ', 'ʊ', 'æ', 'e', 'ə', 'u', 'i', 'ɪ', 'ʌ', 'ɔ', 'ɒ']\n",
    "phon_vowels.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the word as a list of strings representing its clusters\n",
    "# word - word to turn into a list of cluster(letters)-sound(consonant or vowel) pairs\n",
    "# is_orth - true if word is orthological, false is word is phonemic\n",
    "def get_cluster(word, is_orth):\n",
    "    cluster = []\n",
    "    if (is_orth):\n",
    "        consonants = orth_consonants\n",
    "        vowels = orth_vowels\n",
    "    else:\n",
    "        consonants = phon_consonants\n",
    "        vowels = phon_vowels\n",
    "        \n",
    "    for i in range(len(word)):\n",
    "        if (is_orth == True) & (word[i] == 'y'):\n",
    "            # 'y' is a semivowel (can be either vowel or consonant)\n",
    "            if i == 0:\n",
    "                if word[i+1] in consonants:\n",
    "                    cluster.append(([word[i]],'v'))\n",
    "                else:\n",
    "                    cluster.append(([word[i]],'c'))\n",
    "            else:\n",
    "                cluster.append(([word[i]],'v'))\n",
    "        elif word[i] in consonants:\n",
    "            cluster.append(([word[i]],'c'))\n",
    "            \n",
    "        else:\n",
    "            cluster.append(([word[i]],'v'))\n",
    "    \n",
    "    i=0\n",
    "    while i < len(cluster)-1:\n",
    "        if cluster[i][1] == cluster[i+1][1]:\n",
    "            cluster[i] = ((cluster[i][0] + cluster[i+1][0]), cluster[i][1])\n",
    "            cluster.pop(i+1)\n",
    "        else:\n",
    "            i += 1\n",
    "    \n",
    "    return cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3-3'></a>\n",
    "#### Preparing Data with Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_transcriptions = transcriptions.copy()\n",
    "\n",
    "# Apply clustering function to words and transcriptions in dataframe\n",
    "clustered_transcriptions.word = clustered_transcriptions.word.apply(get_cluster, args=(True,))\n",
    "clustered_transcriptions.transcription = clustered_transcriptions.transcription.apply(get_cluster, args=(False,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check compatibility of clustered words and their corresponding transcription\n",
    "def cluster_compatible(word, transcription):\n",
    "    # Are there the same number of clusters?\n",
    "    if len(word) != len(transcription):\n",
    "        return False\n",
    "    \n",
    "    # For each pair of clusters, do they have the same vowel or consonant typing?\n",
    "    for index, tup in enumerate(word):\n",
    "        if tup[1] != transcription[index][1]:\n",
    "            return False\n",
    "    \n",
    "    # Return true if both of these questions are true\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column with the returned value of the cluster_compatible function\n",
    "clustered_transcriptions['compatible'] = clustered_transcriptions.apply(\n",
    "    lambda x: cluster_compatible(x.word, x.transcription), axis=1)\\\n",
    "\n",
    "# Print dataframe to file\n",
    "clustered_transcriptions.to_csv(\"british_clusters.tsv\", sep='\\t')\n",
    "\n",
    "clustered_transcriptions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out compatible tuples for heatmap analysis and incompatible tuples for later analysis\n",
    "compatible_clust = clustered_transcriptions[clustered_transcriptions['compatible'] == True]\n",
    "incompatible_clust = clustered_transcriptions[clustered_transcriptions['compatible'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten each column\n",
    "word_clusters = [clust for row in compatible_clust.word for clust in row]\n",
    "transcription_clusters = [clust for row in compatible_clust.transcription for clust in row]\n",
    "\n",
    "# Join columns together into new flattened dataframe\n",
    "flattened_clust = pd.DataFrame({'transcription': transcription_clusters, 'word': word_clusters})\n",
    "# Create new type column based on typing of current tuple's clusters\n",
    "flattened_clust['type'] = flattened_clust['word'].apply(lambda x: x[1])\n",
    "# Pull out cluster from tuple\n",
    "flattened_clust['word'] = flattened_clust['word'].apply(lambda x: x[0])\n",
    "flattened_clust['transcription'] = flattened_clust['transcription'].apply(lambda x: x[0])\n",
    "\n",
    "flattened_clust.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframes for vowel and consonant clusters for analysis\n",
    "vowel_clusters = flattened_clust[flattened_clust['type'] == 'v']\n",
    "consonant_clusters = flattened_clust[flattened_clust['type'] == 'c']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3-4'></a>\n",
    "#### Intra-cluster Co-occurence Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intra-cluster co-occurrence heatmap was calculated very similarly to the intra-word heatmap, except for the fact that there must be separate heatmaps for the vowels and consonants given this is a specific attribute that each cluster has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vowel_orth_dict = {k: v for v, k in enumerate(orth_vowels)}\n",
    "vowel_phon_dict = {k: v for v, k in enumerate(phon_vowels)}\n",
    "\n",
    "# Create new dataframe for vowel heatmap\n",
    "vowel_heatmap = pd.DataFrame(index=[i for i in range(0, len(vowel_phon_dict))], \n",
    "                             columns=[i for i in range(0, len(vowel_orth_dict))], \n",
    "                             data=0)\n",
    "\n",
    "consonant_orth_dict = {k: v for v, k in enumerate(orth_consonants)}\n",
    "consonant_phon_dict = {k: v for v, k in enumerate(phon_consonants)}\n",
    "\n",
    "# Create new dataframe for consonant heatmap\n",
    "consonant_heatmap = pd.DataFrame(index=[i for i in range(0, len(consonant_phon_dict))], \n",
    "                             columns=[i for i in range(0, len(consonant_orth_dict))], \n",
    "                             data=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run through each cluster and add acording weighted value\n",
    "for index, row in vowel_clusters.iterrows():\n",
    "    word_clust_length = len(row['word'])\n",
    "    transcription_clust_length = len(row['transcription'])\n",
    "    \n",
    "    # Word length weight calculation\n",
    "    weight = word_clust_length * transcription_clust_length\n",
    "    weighted_val = float(1/weight)\n",
    "    \n",
    "    for orth in row['word']:\n",
    "        for phon in row['transcription']:\n",
    "            # Ditribution weight calulation\n",
    "            dist_weight = float(1/(orth_prob[orth] * phon_prob[phon]))\n",
    "            \n",
    "            # Add new weighted value to overal for letter, phoneme pair\n",
    "            vowel_heatmap.loc[[vowel_phon_dict[phon]],[vowel_orth_dict[orth]]] += (weighted_val * dist_weight)\n",
    "\n",
    "# Apply same process to consonant data\n",
    "for index, row in consonant_clusters.iterrows():\n",
    "    word_clust_length = len(row['word'])\n",
    "    transcription_clust_length = len(row['transcription'])\n",
    "    \n",
    "    weight = word_clust_length * transcription_clust_length\n",
    "    weighted_val = float(1/weight)\n",
    "    \n",
    "    for orth in row['word']:\n",
    "        for phon in row['transcription']:\n",
    "            dist_weight = float(1/(orth_prob[orth] * phon_prob[phon]))\n",
    "            \n",
    "            consonant_heatmap.loc[[consonant_phon_dict[phon]],[consonant_orth_dict[orth]]] += (weighted_val * dist_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change index and columns names and normalize dataframes\n",
    "vowel_heatmap.index = phon_vowels\n",
    "vowel_heatmap.columns = orth_vowels\n",
    "\n",
    "vowel_max_val = max(vowel_heatmap.max())\n",
    "vowel_heatmap = vowel_heatmap.div(vowel_max_val)\n",
    "\n",
    "consonant_heatmap.index = phon_consonants\n",
    "consonant_heatmap.columns = orth_consonants\n",
    "\n",
    "consonant_max_val = max(consonant_heatmap.max())\n",
    "consonant_heatmap = consonant_heatmap.div(consonant_max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the dataframe showing the co-occurence values of each phoneme, letter pair\n",
    "plt.subplots(figsize=(20,15))\n",
    "ax_vowel = sns.heatmap(vowel_heatmap, cmap=\"Greens\")\n",
    "plt.yticks(rotation=0, va=\"center\", ha=\"center\")\n",
    "ax_vowel.set_title('Intra-Cluster Co-Occurence of Phoneme and Letter for Vowels')\n",
    "ax_vowel.set_ylabel('Phoneme'), ax_vowel.set_xlabel('Letter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the letter of greatest co-occurence for each phoneme\n",
    "vowel_phon_comax = vowel_heatmap.idxmax(axis=1).to_frame()\n",
    "vowel_phon_comax.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the phoneme of greatest co-occurence for each letter\n",
    "vowel_orth_comax = vowel_heatmap.idxmax(axis=0).to_frame()\n",
    "vowel_orth_comax.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the dataframe showing the co-occurence values of each phoneme, letter pair\n",
    "plt.subplots(figsize=(20,15))\n",
    "ax_consonant = sns.heatmap(consonant_heatmap, cmap=\"Greens\")\n",
    "plt.yticks(rotation=0, va=\"center\", ha=\"center\")\n",
    "ax_consonant.set_title('Intra-Cluster Co-Occurence of Phoneme and Letter for Consonants')\n",
    "ax_consonant.set_ylabel('Phoneme'), ax_consonant.set_xlabel('Letter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the letter of greatest co-occurence for each phoneme\n",
    "consonant_phon_comax = consonant_heatmap.idxmax(axis=1).to_frame()\n",
    "consonant_phon_comax.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the phoneme of greatest co-occurence for each letter\n",
    "consonant_orth_comax = consonant_heatmap.idxmax(axis=0).to_frame()\n",
    "consonant_orth_comax.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our co-occurrence mapping code did a good job at ‘correctly’ mapping phonemes to letters. It is interesting to note that for IPA symbols like ‘ʃ’ (unvoiced postalveolar fricative) and ‘ð’ (voiced interdental fricative) are mapped to the letter ‘h’, which makes sense given these sounds are mostly written as ‘sh’ and ‘th’ respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster manipulation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into learnable chunks\n",
    "def split(word):\n",
    "    out = []\n",
    "    for x in range(len(word)):\n",
    "        current_chunk = []\n",
    "        if x == 0:\n",
    "            current_chunk.append(\"\")\n",
    "        else:\n",
    "            current_chunk.append(\"\".join(word[x-1][0][-2:]))\n",
    "            \n",
    "        current_chunk.append(\"\".join(word[x][0]))\n",
    "        \n",
    "        if x == len(word) - 1:\n",
    "            current_chunk.append(\"\")\n",
    "        else:\n",
    "            current_chunk.append(\"\".join(word[x+1][0][:2]))\n",
    "        \n",
    "        out.append(current_chunk)\n",
    "        \n",
    "    return out\n",
    "\n",
    "# get an array of unannotated string chunks\n",
    "def get_raw_chunks(word):\n",
    "    return [\"\".join(x[0]) for x in word]\n",
    "\n",
    "print(\"Letter chunks in context\")\n",
    "print(split(compatible_clust.iloc[70][\"word\"]))\n",
    "print(\"\\nRaw transcription chunks\")\n",
    "print(get_raw_chunks(compatible_clust.iloc[70][\"transcription\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3-5'></a>\n",
    "#### Incompatible Clustering Analysis \n",
    "\n",
    "Incompatible clustersare those whose cluster-count and polarity (cvcv... vs vcvc…) do not match between spelling and pronunciation. We found approximately 35% of our words incompatible. This can happen for many reasons. The most common is that there is a silent letter at the end, leading to the same polarity but a pronunciation which is one cluster fewer than the spelling. In British english this is most common with words ending in \"e\" or \"r\", as both are often unvoiced. These take up 75% of the words initially labeled incompatible.\n",
    "\n",
    "1. “weather” (cvcvc)  → /weðə/ (cvcv)\n",
    "2. “case” (cvcv)   → /keɪs/ (cvc)\n",
    "\n",
    "Though this would also be the case if the first cluster was silent, the entire first cluster is never silent. One way we thought of combatting this problem was to pad the pronounciation cluster list with a null (unpronounced) character at the end. This way, we could salvage approximately 26% of our dataset for training.\n",
    "\n",
    "One issue with the British dialect of English is that “r” is only pronounced in a specific few cases (ex. “train”, “frame”, “boring”, but not “mark”, “cluster”, and “bored”). For this reason, words which have an “r” in the middle often have a chance of creating a cluster mismatch, giving the pronunciation and the spelling a discrepancy of two clusters. Words like “bored” would not be salvaged by our silent-last-letter algorithm. We are still investigating ways to infer the structure of a semivowel, and this is a place of further research in the future.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incompatible_clust.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints a subset of a word-transcription dataframe\n",
    "def print_n(wordframe, n):\n",
    "    w = list(wordframe[\"word\"].apply(lambda x: get_raw_chunks(x)))\n",
    "    t = list(wordframe[\"transcription\"].apply(lambda x: get_raw_chunks(x)))\n",
    "    for i in range(0, len(w), len(w)//n):\n",
    "        print(\"\\t\"+\"\".join(w[i])+\": \"+\"\".join(t[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# store number of clusters for each word/transcription\n",
    "incompatible_clust.loc[:, \"word_clusters\"] = incompatible_clust['word'].apply(lambda x: len(x))\n",
    "incompatible_clust.loc[:, \"transcription_clusters\"] = incompatible_clust['transcription'].apply(lambda x: len(x))\n",
    "\n",
    "# get the percent of all words which are cluster incompatible\n",
    "percent_incompatible = len(incompatible_clust)/len(clustered_transcriptions)\n",
    "\n",
    "# store the polarities of each incompatible word\n",
    "word_polarities = incompatible_clust['word'].apply(lambda x: x[0][1])\n",
    "trans_polarities = incompatible_clust['transcription'].apply(lambda x: x[0][1])\n",
    "# get the percent of incompatible words which have the same polarity\n",
    "same_start_percent = len(incompatible_clust[word_polarities == trans_polarities])/len(incompatible_clust)\n",
    "\n",
    "# split into the set of same polarity and opposite polarity words\n",
    "opp_start = incompatible_clust[word_polarities != trans_polarities]\n",
    "same_start = incompatible_clust[word_polarities == trans_polarities]\n",
    "\n",
    "# print some opposite polarity words\n",
    "print(\"Words which do not share polarity\")\n",
    "print_n(opp_start, 6)\n",
    "print()\n",
    "\n",
    "# Find all words with one fewer phono cluster than cluster (silent last letter)\n",
    "trans_one_shorter = same_start['word_clusters'] == same_start['transcription_clusters']+1\n",
    "percent_trans_one_shorter = len(same_start[trans_one_shorter])/len(incompatible_clust)\n",
    "\n",
    "print(\"Words which have a silent last letter\")\n",
    "print_n(same_start[trans_one_shorter], 6)\n",
    "print()\n",
    "\n",
    "trans_two_shorter = same_start['word_clusters'] == same_start['transcription_clusters']+2\n",
    "percent_trans_two_shorter = len(same_start[trans_two_shorter])/len(incompatible_clust)\n",
    "print(\"Words which have a silent middle cluster\")\n",
    "print_n(same_start[trans_two_shorter], 6)\n",
    "\n",
    "plt.pie([1-percent_incompatible, \n",
    "         percent_incompatible*percent_trans_one_shorter,\n",
    "         percent_incompatible*percent_trans_two_shorter,\n",
    "         percent_incompatible*(same_start_percent - percent_trans_one_shorter - percent_trans_two_shorter), \n",
    "         percent_incompatible*(1-same_start_percent)], \n",
    "        labels=[\"Compatible\", \"Silent Last Cluster\", \"Silent Middle Cluster\", \"Misc Incompatible\", \"Different cluster polarity\"],\n",
    "        colors=[\"blue\", \"green\", \"orange\", \"yellow\", \"red\"])\n",
    "plt.show()\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "### Decision Tree Model\n",
    "\n",
    "After having separated the words into localized data, we were able to analyze it. We know that individual letters can change pronunciation based on the letters around them (ex. The “a” in “ban” vs. “bad”), and are less affected by the clusters far away (\"basket\" vs. \"basker\"), and the clustering allowed us to localize the effects of clusters. Our goal is to predict the pronunciation of an entire cluster based on its contents one the contents around it. For this we trained a Decision Tree classifier on the nearest two letters of the clusters surrounding the target cluster, and on the target cluster itself. The decision for two letters was motivated by the fact that letter combinations of two can produce wildly different sounds than their last letter taken alone ( “h” vs. “th,” “ch,” “ah”).\n",
    "\n",
    "We chose a decision tree model because it seems consistent with the method by which children learn to pronounce words. They are often given a set of rules and told to apply them to a spelling to find its pronounciation. To do so, we one-hot encoded each cluster and the surrounding two clusters, and fed that into our decision tree predictor. We do believe that there are more layers of steps and filters which decrease the likelihood of some phoneme structures, but this as a beginning is shockingly effective.\n",
    "\n",
    "One issue with this is that it treats each cluster as a completely separate entity from the letters which comprise it. This has its benefits, especially in vowel clusters ('oo' != 'o'), but means that much more data needs to be taken in to understand and recognize every cluster. This also means that there is a high chance a cluster will appear which has not been seen before, and the Decision Tree model has no way of dealing with this. Though we do believe that a rule based system is present in the transformation from spelling to pronunciation, we believe that it is part of a yet to be understood larger process. However, on the dataset we had, our decision tree was very effective, and provides a promising starting point for future research.\n",
    "\n",
    "Our decision tree has an average of 80% accuracy when tested with 10 fold validation 100 times. This we think is a success, considering how we know that some amount of pronunciation is based on memorization. In the future, we think this data model can be expanded to more robustly and narrowly predict pronunciations of many languages based on cross-linguistic information (like phonemic features).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess data\n",
    "X = []\n",
    "y = []\n",
    "for i, word in compatible_clust.iterrows():\n",
    "    X += split(word[\"word\"])\n",
    "    y += get_raw_chunks(word[\"transcription\"])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(\"Input (X):\")\n",
    "print(X)\n",
    "print(\"\\nCorresponding Output (y):\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as DecisionTree\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# create encoders to store data binarily\n",
    "letter_encoder = OneHotEncoder()\n",
    "letter_encoder.fit(X)\n",
    "X_p = letter_encoder.transform(X).toarray()\n",
    "\n",
    "phono_encoder  = OneHotEncoder()\n",
    "phono_encoder.fit(np.reshape(y,(-1,1)))\n",
    "y_p = phono_encoder.transform(np.reshape(y,(-1,1))).toarray()\n",
    "\n",
    "print(\"ONE HOT ENCODING:\\n\"+\"-\"*50)\n",
    "print(\"X:\")\n",
    "print(X)\n",
    "print(\"X encoded:\")\n",
    "print(X_p)\n",
    "print()\n",
    "print(\"y:\")\n",
    "print(y)\n",
    "print(\"y encoded:\")\n",
    "print(y_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to calculate n-fold validation!\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "# generates a random ordering of [0..n]\n",
    "#   where n = len(arr)\n",
    "def get_shuf_ordering(arr):\n",
    "    r = list(range(len(arr)))\n",
    "    shuffle(r)\n",
    "    return r\n",
    "\n",
    "# shuffles an array by rearranging it to an ordering\n",
    "# \n",
    "# an ordering is an arrangement of [0..n]\n",
    "#\n",
    "# output[ordering[i]] = input[i]\n",
    "#\n",
    "def shuf(arr, ordering):\n",
    "    assert len(arr) == len(ordering)\n",
    "    assert max(ordering) == len(arr)-1\n",
    "    assert min(ordering) == 0\n",
    "    \n",
    "    out = np.zeros(arr.shape, dtype=arr.dtype)\n",
    "    \n",
    "    for inloc, oloc in enumerate(ordering):\n",
    "        out[oloc] = arr[inloc]\n",
    "    \n",
    "    return out\n",
    "\n",
    "# Randomizes a pair of arrays X and y, while\n",
    "#   maintaining their relative ordering\n",
    "def randomize_Xy(X, y):\n",
    "    r_ordering = get_shuf_ordering(X)\n",
    "\n",
    "    X_r = shuf(X, r_ordering)\n",
    "    y_r = shuf(y, r_ordering)\n",
    "    \n",
    "    return X_r, y_r\n",
    "\n",
    "# randomize X and y, then split them\n",
    "#   into a training set and test set\n",
    "#\n",
    "# X    = input data matrix\n",
    "# y    = output value array\n",
    "# fold = the n of the n-fold validation\n",
    "def x_fold(X, y, fold):\n",
    "    # index of 1/fold of the data\n",
    "    percent = int(len(X)/fold)\n",
    "    \n",
    "    X_r, y_r = randomize_Xy(X, y)\n",
    "    \n",
    "    # all values before percent\n",
    "    validate_X = X_r[:percent]\n",
    "    validate_y = y_r[:percent]\n",
    "    \n",
    "    # all values after percent\n",
    "    train_X = X_r[percent:]\n",
    "    train_y = y_r[percent:]\n",
    "    \n",
    "    return train_X, train_y, validate_X, validate_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up average and number of iteratons\n",
    "avg = 0\n",
    "iterations = 100\n",
    "\n",
    "for _ in range(iterations):\n",
    "    train_X, train_y, validation_X, validation_y = x_fold(X_p, y_p, 10)\n",
    "    \n",
    "    tree = DecisionTree().fit(train_X, train_y)\n",
    "    \n",
    "    v_score = tree.score(validation_X, validation_y)\n",
    "    \n",
    "    avg += v_score\n",
    "\n",
    "print(\"Accuracy score: %.01f%%\"%(100*avg/iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTree().fit(X_p, y_p)\n",
    "\n",
    "def predict(word):\n",
    "    word = np.array(split(word))\n",
    "    word_p = letter_encoder.transform(word)\n",
    "    \n",
    "    return \"\".join([x[0] for x in phono_encoder.inverse_transform(dtree.predict(word_p))])\n",
    "\n",
    "print(\"Some word predictions:\\n\")\n",
    "\n",
    "print(\"Accurate predictions:\")\n",
    "print(\"\\tbasket: \"+predict(\"basket\"))\n",
    "print(\"\\toven: \"+predict(\"oven\"))\n",
    "print(\"\\tpasta: \"+predict(\"pasta\"))\n",
    "print(\"Inaccurate predictions\")\n",
    "print(\"\\tfrisbee: \"+predict(\"frisbee\"))\n",
    "print(\"\\tartist: \"+predict(\"artist\"))\n",
    "print(\"\\tcomputational: \"+predict(\"computational\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "### Ideas for Further Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>More data! The dataset we focused on is standard British English, also called the Queen’s English. Although this dialect is classified as the standard, the British Isles is made up of many different accents and dialects – more than 37 dialects at the last count. Dialects from other regions of Britain include: Cockney, Estuary, Yorkshire, Northern Irish, Scottish, and more!</li>\n",
    "<li>A more robust model. We currently treat each cluster as a separate entity not relating to its internal letters, which means that we cannot generalize to new clusters. This is an important area to research.</li>\n",
    "<li>Explaining data which does not match our clustering model. We hope in the future to explain ways to predict the edge-case words that are not modeled by our cluster analysis, as it’s clearly possible to predict their pronunciation by humans. This often comes down to elided clusters, but that is not the only case, and in exploring the edge cases we believe we will come to a more thorough understanding of the process in general.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6'></a>\n",
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our favorable results, we were still bound by certain specifications, such as focusing on one language and having the predicate knowledge of some linguistic rules (knowing the alphabet and what constitutes a consonant or vowel). There are still many factors to consider when creating a prediction on a word’s pronunciation, including human interpretation, language dialects, specific rules, and even sub-regional accents that are difficult to generalize outside of our dataset. Even with these limitations, however, there is still huge potential for expansion outside of the British English dataset we focused on.\n",
    "\n",
    "Currently, there is a lack of thorough research in relationships amongst different languages as to ways which spelling and pronunciation are related. Each language itself has tried to cotify rules for pronunciation, including <a href=\"https://en.wikipedia.org/wiki/English_orthography#Combinations_of_vowel_letters\">vowel clusters</a> in English orthography, though there hasn’t been any cross-linguistic research, as it’s yet to be explored.\n",
    "\n",
    "Ultimately, some part of pronunciation is memorization and personal interpretation. Without any given context of a word that is new or unfamiliar to the language, different people would interpret the pronunciation of the word differently in their minds. For example, if we were to make up a word in English and provide no context, two people who speak the same language in exactly the same dialect, could potentially pronounce this new word completely different. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
